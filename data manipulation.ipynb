{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e6e684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Aper√ßu des 10 premi√®res lignes du CSV :\n",
      "\n",
      " Unnamed: 0  Rank (Borda)                                                                                            Model Zero-shot Memory Usage (MB) Number of Parameters Embedding Dimensions Max Tokens  Mean (Task)  Mean (TaskType)  Bitext Mining  Classification  Clustering  Instruction Reranking  Multilabel Classification  Pair Classification  Reranking  Retrieval   STS\n",
      "          0             1                 [llama-embed-nemotron-8b](https://huggingface.co/nvidia/llama-embed-nemotron-8b)       99%             28629                   7B                 4096      32768        69.46            61.09          81.72           73.21       54.35                  10.82                      29.86                83.97      67.78      68.69 79.41\n",
      "          1             2                         [gemini-embedding-001](https://ai.google.dev/gemini-api/docs/embeddings)       99%           Unknown              Unknown                 3072       2048        68.37            59.59          79.28           71.82       54.59                   5.18                      29.16                83.63      65.58      67.71 79.40\n",
      "          2             3                             [Qwen3-Embedding-8B](https://huggingface.co/Qwen/Qwen3-Embedding-8B)       99%             28866                   7B                 4096      32768        70.58            61.69          80.89           74.00       57.65                  10.06                      28.66                86.40      65.63      70.88 81.08\n",
      "          3             4                             [Qwen3-Embedding-4B](https://huggingface.co/Qwen/Qwen3-Embedding-4B)       99%             15341                   4B                 2560      32768        69.45            60.86          79.36           72.33       57.15                  11.56                      26.77                85.05      65.08      69.60 80.86\n",
      "          4             5                         [Qwen3-Embedding-0.6B](https://huggingface.co/Qwen/Qwen3-Embedding-0.6B)       99%              2272                 595M                 1024      32768        64.34            56.01          72.23           66.83       52.33                   5.09                      24.59                80.83      61.41      64.65 76.17\n",
      "          5             6                [gte-Qwen2-7B-instruct](https://huggingface.co/Alibaba-NLP/gte-Qwen2-7B-instruct)     ‚ö†Ô∏è NA             29040                   7B                 3584      32768        62.51            55.93          73.92           61.55       52.77                   4.94                      25.48                85.13      65.55      60.08 73.98\n",
      "          6             7                 [Linq-Embed-Mistral](https://huggingface.co/Linq-AI-Research/Linq-Embed-Mistral)       99%             13563                   7B                 4096      32768        61.47            54.14          70.34           62.24       50.60                   0.94                      24.77                80.43      64.37      58.69 74.86\n",
      "          7             8 [multilingual-e5-large-instruct](https://huggingface.co/intfloat/multilingual-e5-large-instruct)       99%              1068                 560M                 1024        514        63.22            55.08          80.13           64.94       50.75                  -0.40                      22.91                80.86      62.61      57.12 76.81\n",
      "          8             9                [embeddinggemma-300m](https://ai.google.dev/gemma/docs/embeddinggemma/model_card)       99%               578                 307M                  768       2048        61.15            54.31          64.40           60.90       51.17                   5.61                      24.82                81.40      63.25      62.49 74.73\n",
      "          9            10                 [SFR-Embedding-Mistral](https://huggingface.co/Salesforce/SFR-Embedding-Mistral)       96%             13563                   7B                 4096      32768        60.90            53.92          70.00           60.02       51.84                   0.16                      24.55                80.29      64.19      59.44 74.79\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# üîß R√©glages d'affichage\n",
    "pd.set_option('display.max_columns', None)   # Affiche toutes les colonnes\n",
    "pd.set_option('display.width', 0)            # Ne coupe pas les lignes\n",
    "pd.set_option('display.max_colwidth', None)  # Affiche tout le texte d'une cellule\n",
    "\n",
    "# üìÇ Lecture du CSV\n",
    "data = pd.read_csv('benchmark_data.csv', delimiter=',')\n",
    "\n",
    "# üëÄ Aper√ßu structur√©\n",
    "print(\"\\nüìä Aper√ßu des 10 premi√®res lignes du CSV :\\n\")\n",
    "print(data.head(10).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634f5ecc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
